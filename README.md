# Stallion Motors — Sales Performance Dashboard (Synthetic Project)
# OGAR PHILEMON - DATA ANALYST

## Overview
I have created This project to simulates sales data for **Stallion Motors** and provides:
- A synthetic dataset: `stallion_sales_data.csv`
- EDA notebook: `sales_analysis.ipynb`
- Forecasting notebook (6-month): `sales_forecast.ipynb`
- Streamlit dashboard app: `app.py`

## Files
- `stallion_sales_data.csv` — Synthetic transaction-level sales data (2022-01-01 to 2025-10-31).
- `stallion_monthly_agg.csv` — Monthly aggregates (generated by `sales_analysis.ipynb`).
- `stallion_prophet_forecast.csv` — Prophet forecast output (generated by `sales_forecast.ipynb`).
- `stallion_arima_forecast.csv` — ARIMA forecast output.
- `app.py` — Streamlit dashboard. Run with `streamlit run app.py`.

## Quick Start
1. Create a virtual environment (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # macOS / Linux
   venv\Scripts\activate   # Windows
   ```

2. Install requirements:
   ```bash
   pip install pandas numpy plotly streamlit prophet pmdarima
   ```

   - If `prophet` installation fails, you can use `pmdarima` (ARIMA) as fallback in `sales_forecast.ipynb`.

3. Run EDA notebook to create monthly aggregates:
   - Open `sales_analysis.ipynb` in Jupyter and run all cells. This will create `stallion_monthly_agg.csv`.

4. Run forecasting:
   - Open `sales_forecast.ipynb` and run all cells. This will create `stallion_prophet_forecast.csv` and `stallion_arima_forecast.csv`.

5. Run the Streamlit app:
   ```bash
   streamlit run app.py
   ```
   - The sidebar contains filters. To see the forecast, ensure `stallion_prophet_forecast.csv` exists (generate it from the forecasting notebook).

## Features & Ideas to Extend
- Add salesperson ranking with commission calculation.
- Include lead source (online, walk-in, referral) and conversion funnel.
- Add inventory tracking and delivery status.
- Build an automated pipeline to regenerate forecasts monthly.

## Notes
- The dataset is synthetic and designed for portfolio/demo use.
- Date range ends on 2025-10-31.



## New: Commission calculations, Salesperson Leaderboard & Automatic Pipeline

### Commission rules (example)
- Base commission = 3% of **Profit** per sale.
- Returning customer bonus = $50 per sale.
- Monthly revenue tier bonuses for salespeople:
  - >= $150,000 → extra 2% of revenue
  - >= $100,000 → extra 1.2% of revenue
  - >= $50,000 → extra 0.6% of revenue

These rules are implemented in `commission_calc.py`. Adjust constants inside that file to tune the policy.

### Pipeline: regenerate aggregates, leaderboard, and forecasts
Run the pipeline script to automatically regenerate:
```bash
python run_pipeline.py --sales_csv stallion_sales_data.csv
```
This will produce/overwrite:
- `stallion_monthly_agg.csv`
- `salesperson_leaderboard_monthly.csv`
- `stallion_prophet_forecast.csv` (if Prophet installed)
- `stallion_arima_forecast.csv` (fallback if Prophet unavailable)

The Streamlit app (`app.py`) will automatically display the latest leaderboard and commission estimates when `commission_calc.py` is present.


## Scheduling & Automation (Expanded)

Below are practical examples to automate the monthly pipeline.

### Cron (Linux / macOS)
Example cron entry to run pipeline at 02:30 AM UTC on the 1st of every month:

```
# Edit crontab: crontab -e
30 2 1 * * cd /path/to/your/project && /path/to/venv/bin/python run_pipeline.py --sales_csv stallion_sales_data.csv >> pipeline.log 2>&1
```

- Replace `/path/to/your/project` with the folder containing the project files.
- Replace `/path/to/venv/bin/python` with your Python interpreter (or simply `python` if using system Python).
- Logs will write to `pipeline.log`—rotate or archive logs appropriately.
- To test manually, run:
```
python run_pipeline.py --sales_csv stallion_sales_data.csv
```

### GitHub Actions (CI) — included file
A GitHub Actions workflow is included at `.github/workflows/monthly_pipeline.yml`.
It runs on the 1st of every month (02:30 UTC) and on manual dispatch. It installs dependencies and runs the pipeline.

**How to use:**
1. Commit the repository to GitHub.
2. Adjust `monthly_pipeline.yml` if you use different Python versions or private package indices.
3. Check workflow runs in the "Actions" tab; downloaded artifacts contain the generated CSVs.

### Tips for production scheduling
- Use secrets for any private credentials (e.g., S3 uploads, database access) via GitHub Actions secrets.
- Consider adding a step to push outputs to a storage bucket (S3 / GCS) or to create a Git tag and commit results back to the repo (careful with repo write permissions).
- Use job concurrency and protected branches to control runs.
